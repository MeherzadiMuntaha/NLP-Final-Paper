{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ac5c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fdaf650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation and stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        words = [word.lower() for word in words if word.isalpha()]\n",
    "        words = [word for word in words if not word in stop_words]\n",
    "        preprocessed_sentence = ' '.join(words)\n",
    "        preprocessed_sentences.append(preprocessed_sentence)\n",
    "    \n",
    "    return preprocessed_sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "980a0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Engineering\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def extract_features(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    return X\n",
    "\n",
    "# Step 3: Sentence Scoring\n",
    "# You need to implement your scoring mechanism here\n",
    "\n",
    "# Step 4: Training a Decision Tree Model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "026c5442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30b3547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Summarization\n",
    "def summarize_text(text, clf, vectorizer):\n",
    "    preprocessed_sentences = preprocess_text(text)\n",
    "    X = vectorizer.transform(preprocessed_sentences)\n",
    "    scores = clf.predict(X)\n",
    "    \n",
    "    # Select top sentences based on scores\n",
    "    top_sentences_indices = scores.argsort()[-3:][::-1]  # Example: Select top 3 sentences\n",
    "    summary = [preprocessed_sentences[i] for i in top_sentences_indices]\n",
    "    return ' '.join(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "448756ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\Admin\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate.tar.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load sample dataset\u001b[39;00m\n\u001b[0;32m      6\u001b[0m categories \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrec.sport.baseball\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msci.space\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m newsgroups_train \u001b[38;5;241m=\u001b[39m fetch_20newsgroups(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, categories\u001b[38;5;241m=\u001b[39mcategories)\n\u001b[0;32m      8\u001b[0m newsgroups_test \u001b[38;5;241m=\u001b[39m fetch_20newsgroups(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, categories\u001b[38;5;241m=\u001b[39mcategories)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Extract text and labels\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:284\u001b[0m, in \u001b[0;36mfetch_20newsgroups\u001b[1;34m(data_home, subset, categories, shuffle, random_state, remove, download_if_missing, return_X_y)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download_if_missing:\n\u001b[0;32m    283\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading 20news dataset. This may take a few minutes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 284\u001b[0m     cache \u001b[38;5;241m=\u001b[39m _download_20newsgroups(\n\u001b[0;32m    285\u001b[0m         target_dir\u001b[38;5;241m=\u001b[39mtwenty_home, cache_path\u001b[38;5;241m=\u001b[39mcache_path\n\u001b[0;32m    286\u001b[0m     )\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20Newsgroups dataset not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:80\u001b[0m, in \u001b[0;36m_download_20newsgroups\u001b[1;34m(target_dir, cache_path)\u001b[0m\n\u001b[0;32m     78\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecompressing \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, archive_path)\n\u001b[0;32m     79\u001b[0m tarfile\u001b[38;5;241m.\u001b[39mopen(archive_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr:gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mextractall(path\u001b[38;5;241m=\u001b[39mtarget_dir)\n\u001b[1;32m---> 80\u001b[0m os\u001b[38;5;241m.\u001b[39mremove(archive_path)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Store a zipped pickle\u001b[39;00m\n\u001b[0;32m     83\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     84\u001b[0m     train\u001b[38;5;241m=\u001b[39mload_files(train_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     85\u001b[0m     test\u001b[38;5;241m=\u001b[39mload_files(test_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     86\u001b[0m )\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\Admin\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate.tar.gz'"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "    # Load sample dataset\n",
    "    categories = ['rec.sport.baseball', 'sci.space']\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "    newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "    # Extract text and labels\n",
    "    X_train, y_train = newsgroups_train.data, newsgroups_train.target\n",
    "    X_test, y_test = newsgroups_test.data, newsgroups_test.target\n",
    "\n",
    "    # Preprocess text\n",
    "    preprocessed_train = [preprocess_text(text) for text in X_train]\n",
    "    preprocessed_test = [preprocess_text(text) for text in X_test]\n",
    "\n",
    "    # Flatten the preprocessed lists\n",
    "    preprocessed_train = [sentence for sublist in preprocessed_train for sentence in sublist]\n",
    "    preprocessed_test = [sentence for sublist in preprocessed_test for sentence in sublist]\n",
    "\n",
    "    # Extract features\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train_features = vectorizer.fit_transform(preprocessed_train)\n",
    "    X_test_features = vectorizer.transform(preprocessed_test)\n",
    "\n",
    "    # Train decision tree\n",
    "    clf = train_decision_tree(X_train_features, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy, report = evaluate(clf, X_test_features, y_test)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5485699",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (3795819384.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[18], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    As an AI language model, I can provide you with a Python code framework for summarizing English text using sentence scoring and a decision tree approach. However, please note that creating a fully functional system would require more detailed implementation and tuning, as well as access to specific datasets. Here's a basic outline of how you can approach it:\u001b[0m\n\u001b[1;37m                                                                                                                                                                                                                                                                                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee7c57c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return sentences, words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0aa236c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_scores(sentences, words):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    word_matrix = vectorizer.fit_transform(words)\n",
    "    word_features = vectorizer.get_feature_names_out()\n",
    "    sentence_scores = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_words = word_tokenize(sentence.lower())\n",
    "        sentence_words = [word for word in sentence_words if word.isalnum() and word not in stop_words]\n",
    "        score = 0\n",
    "        for word in sentence_words:\n",
    "            if word in word_features:\n",
    "                score += word_matrix[0, word_features.index(word)]\n",
    "        sentence_scores.append(score)\n",
    "\n",
    "    return sentence_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05fdab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(sentences, sentence_scores, threshold=0.5):\n",
    "    avg_score = sum(sentence_scores) / len(sentence_scores)\n",
    "    summary = \"\"\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        if sentence_scores[i] >= avg_score * threshold:\n",
    "            summary += sentences[i] + \" \"\n",
    "\n",
    "    return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2dd4419",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rouge'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrouge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rouge\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_rouge\u001b[39m(reference_summary, generated_summary):\n\u001b[0;32m      4\u001b[0m     rouge \u001b[38;5;241m=\u001b[39m Rouge()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rouge'"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "def calculate_rouge(reference_summary, generated_summary):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(generated_summary, reference_summary)\n",
    "    return scores\n",
    "\n",
    "def main():\n",
    "    # Example reference summary\n",
    "    reference_summary = \"\"\"\n",
    "    This is the reference summary against which the generated summary will be evaluated.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generated summary from the previous code\n",
    "    generated_summary = \"\"\"\n",
    "    This is the generated summary that you want to evaluate.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate ROUGE scores\n",
    "    rouge_scores = calculate_rouge(reference_summary, generated_summary)\n",
    "\n",
    "    # Print ROUGE scores\n",
    "    print(\"ROUGE Scores:\")\n",
    "    print(rouge_scores)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09c5968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
